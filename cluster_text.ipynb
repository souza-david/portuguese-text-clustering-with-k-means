{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53c6cc2-2369-4761-9981-2fe9c16b56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb51e9f-caf0-4537-b49c-772f05524d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structures\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "\n",
    "# Corpus Processing\n",
    "import re\n",
    "import nltk.corpus\n",
    "from unidecode                        import unidecode\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "from nltk                             import SnowballStemmer\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "\n",
    "# K-Means\n",
    "from sklearn import cluster\n",
    "\n",
    "# Visualization and Analysis\n",
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb0b5f-c6b0-4afb-ad83-f91cc142493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus Loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f321080-6637-448d-b78c-a1878175c308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/extracted_text.csv', encoding='utf-8')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6c7ef-92b9-42a3-a437-05bd1b350d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['Comentário']) # Remove NaN cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84713d7-d23b-48d5-ab2b-f355c9494385",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = map(str.lower, data.columns)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a5484-a9c0-464c-ae5c-569dc200723d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = data['comentário'].tolist()\n",
    "corpus[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee124b-dde3-4cc7-b125-7df428014121",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Corpus Processing\n",
    "\n",
    "### 1. Stop Words and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1dfc1-baa6-4975-918e-307dd47844b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removes a list of words (ie. stopwords) from a tokenized list.\n",
    "def removeWords(listOfTokens, listOfWords):\n",
    "    return [token for token in listOfTokens if token not in listOfWords]\n",
    "\n",
    "# applies stemming to a list of tokenized words\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n",
    "\n",
    "# removes any words composed of less than 2 or more than 21 letters\n",
    "def twoLetters(listOfTokens):\n",
    "    twoLetterWord = []\n",
    "    for token in listOfTokens:\n",
    "        if len(token) <= 2 or len(token) >= 21:\n",
    "            twoLetterWord.append(token)\n",
    "    return twoLetterWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d55247c-1f35-4202-855f-4b817548fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. The main corpus processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd70ff-2f36-40b4-8639-9307cafa1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processCorpus(corpus, language):\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    param_stemmer = SnowballStemmer(language)\n",
    "    positive_praise = [line.rstrip('\\n') for line in open('lists/positive_praise.txt')] # Load .txt file line by line\n",
    "    other_words = [line.rstrip('\\n') for line in open('lists/stopwords_scrapmaker.txt')] # Load .txt file line by line\n",
    "\n",
    "    for document in corpus:\n",
    "        index = corpus.index(document)\n",
    "        corpus[index] = corpus[index].replace(u'\\ufffd', '8')   # Replaces the ASCII '�' symbol with '8'\n",
    "        corpus[index] = corpus[index].replace(',', '')          # Removes commas\n",
    "        corpus[index] = corpus[index].rstrip('\\n')              # Removes line breaks\n",
    "        corpus[index] = corpus[index].casefold()                # Makes all letters lowercase\n",
    "        \n",
    "        corpus[index] = re.sub('\\W_',' ', corpus[index])        # removes specials characters and leaves only words\n",
    "        corpus[index] = re.sub(\"\\S*\\d\\S*\",\" \", corpus[index])   # removes numbers and words concatenated with numbers IE h4ck3r. Removes road names such as BR-381.\n",
    "        corpus[index] = re.sub(\"\\S*@\\S*\\s?\",\" \", corpus[index]) # removes emails and mentions (words with @)\n",
    "        corpus[index] = re.sub(r'http\\S+', '', corpus[index])   # removes URLs with http\n",
    "        corpus[index] = re.sub(r'www\\S+', '', corpus[index])    # removes URLs with www\n",
    "\n",
    "        listOfTokens = word_tokenize(corpus[index])\n",
    "        twoLetterWord = twoLetters(listOfTokens)\n",
    "\n",
    "        listOfTokens = removeWords(listOfTokens, stopwords)\n",
    "        listOfTokens = removeWords(listOfTokens, twoLetterWord)\n",
    "        listOfTokens = removeWords(listOfTokens, positive_praise)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "        \n",
    "        listOfTokens = applyStemming(listOfTokens, param_stemmer)\n",
    "        listOfTokens = removeWords(listOfTokens, other_words)\n",
    "\n",
    "        corpus[index]   = \" \".join(listOfTokens)\n",
    "        corpus[index] = unidecode(corpus[index])\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3b557-2fd0-402a-97d8-5ae4bf56af60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "language = 'portuguese'\n",
    "corpus = processCorpus(corpus, language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4a8f7-b4b8-436e-a9ce-aff9cccc09a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f8be9-3b00-4d0e-9fe4-3d03067c4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(filter(None, corpus)) # remove empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ce666-c748-4e9b-8d02-7c7542ca4960",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d11b50-99dd-4a8a-b55d-fd07d9c263f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Statistical Weighting of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5521053-922a-473b-9eac-1a42905f73fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "final_df = tf_idf\n",
    "\n",
    "print(\"{} rows\".format(final_df.shape[0]))\n",
    "final_df.T.nlargest(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63d287-2ca4-4b18-a4d1-44f876e01c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 words with highest weight on document 0:\n",
    "final_df.T.nlargest(5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5de04a-8db1-4d44-a59e-314f75af51d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## K-Means\n",
    "\n",
    "##### Function that runs the K-Means algorithm *max_k* times and returns a dictionary of each k result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c3a75-0d8d-4618-b674-f0d46efcad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_KMeans(max_k, data):\n",
    "    max_k += 1\n",
    "    kmeans_results = dict()\n",
    "    for k in range(2 , max_k):\n",
    "        kmeans = cluster.KMeans(n_clusters = k\n",
    "                               , init = 'k-means++'\n",
    "                               , n_init = 10\n",
    "                               , tol = 0.0001\n",
    "                               , random_state = 1\n",
    "                               , algorithm = 'full')\n",
    "\n",
    "        kmeans_results.update( {k : kmeans.fit(data)} )\n",
    "        \n",
    "    return kmeans_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5734fbfc-ea89-49f1-827d-0afffe52b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Silhouette Score\n",
    "\n",
    "##### The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c3a675-09b4-44ed-95d3-2a80064aeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printAvg(avg_dict):\n",
    "    for avg in sorted(avg_dict.keys(), reverse=True):\n",
    "        print(\"Avg: {}\\tK:{}\".format(avg.round(4), avg_dict[avg]))\n",
    "        \n",
    "def plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg):\n",
    "    fig, ax1 = plt.subplots(1)\n",
    "    fig.set_size_inches(8, 6)\n",
    "    ax1.set_xlim([-0.2, 1])\n",
    "    ax1.set_ylim([0, len(df) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\") # The vertical line for average silhouette score of all the values\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    plt.title((\"Silhouette analysis for K = %d\" % n_clusters), fontsize=10, fontweight='bold')\n",
    "    \n",
    "    y_lower = 10\n",
    "    sample_silhouette_values = silhouette_samples(df, kmeans_labels) # Compute the silhouette scores for each sample\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[kmeans_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Label the silhouette plots with their cluster numbers at the middle\n",
    "        y_lower = y_upper + 10  # Compute the new y_lower for next plot. 10 for the 0 samples\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "def silhouette(kmeans_dict, df, plot=False):\n",
    "    df = df.to_numpy()\n",
    "    avg_dict = dict()\n",
    "    for n_clusters, kmeans in kmeans_dict.items():      \n",
    "        kmeans_labels = kmeans.predict(df)\n",
    "        silhouette_avg = silhouette_score(df, kmeans_labels) # Average Score for all Samples\n",
    "        avg_dict.update( {silhouette_avg : n_clusters} )\n",
    "    \n",
    "        if(plot): plotSilhouette(df, n_clusters, kmeans_labels, silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c77de-653f-464f-a194-095640236d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Kmeans\n",
    "k = 8\n",
    "kmeans_results = run_KMeans(k, final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dc7ad1-36f3-42de-8e71-2c83b4a01825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Silhouette Analysis\n",
    "silhouette(kmeans_results, final_df, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ddf94c-7929-47ee-a4fb-7215f61e8739",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1238d99-529b-4721-9faa-e0d5bf625441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_features_cluster(tf_idf_array, prediction, n_feats):\n",
    "    labels = np.unique(prediction)\n",
    "    dfs = []\n",
    "    for label in labels:\n",
    "        id_temp = np.where(prediction==label) # indices for each cluster\n",
    "        x_means = np.mean(tf_idf_array[id_temp], axis = 0) # returns average score across cluster\n",
    "        sorted_means = np.argsort(x_means)[::-1][:n_feats] # indices with top 20 scores\n",
    "        features = vectorizer.get_feature_names()\n",
    "        best_features = [(features[i], x_means[i]) for i in sorted_means]\n",
    "        df = pd.DataFrame(best_features, columns = ['features', 'score'])\n",
    "        dfs.append(df)\n",
    "    return dfs\n",
    "\n",
    "def plotWords(dfs, n_feats):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i in range(0, len(dfs)):\n",
    "        plt.title((\"Most Common Words in Cluster {}\".format(i)), fontsize=10, fontweight='bold')\n",
    "        sns.barplot(x = 'score' , y = 'features', orient = 'h' , data = dfs[i][:n_feats])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcbf04-64e7-4a8a-8313-b55b3c975c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_result = 5\n",
    "kmeans = kmeans_results.get(best_result)\n",
    "\n",
    "final_df_array = final_df.to_numpy()\n",
    "prediction = kmeans.predict(final_df)\n",
    "n_feats = 20\n",
    "dfs = get_top_features_cluster(final_df_array, prediction, n_feats)\n",
    "plotWords(dfs, 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f03d9-5fdc-4d32-9075-1d2d63b2857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Map of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972507b0-c761-4a1a-b9e3-d722260ff44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms a centroids dataframe into a dictionary to be used on a WordCloud.\n",
    "def centroidsDict(centroids, index):\n",
    "    a = centroids.T[index].sort_values(ascending = False).reset_index().values\n",
    "    centroid_dict = dict()\n",
    "\n",
    "    for i in range(0, len(a)):\n",
    "        centroid_dict.update( {a[i,0] : a[i,1]} )\n",
    "\n",
    "    return centroid_dict\n",
    "\n",
    "def generateWordClouds(centroids):\n",
    "    wordcloud = WordCloud(max_font_size=100, background_color = 'white')\n",
    "    for i in range(0, len(centroids)):\n",
    "        centroid_dict = centroidsDict(centroids, i)        \n",
    "        wordcloud.generate_from_frequencies(centroid_dict)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.title('Cluster {}'.format(i))\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9cd7a2-6992-4652-a431-ae95140b037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pd.DataFrame(kmeans.cluster_centers_)\n",
    "centroids.columns = final_df.columns\n",
    "generateWordClouds(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865792c6-db24-4dd3-a8e7-2c571d5e859b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
